{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Abdul's Notes","text":"<p>A compilation of my notes that I have written throughout my undergraduate career at UC Davis.</p>"},{"location":"ECS%20158%20-%20Programming%20on%20Parallel%20Architectures/","title":"ECS 158   Programming on Parallel Architectures","text":"<p>In preparation for the upcoming midterm, I have prepared a compilation of notes from Professor Weaver's lectures, please let me know if there's anything confusing about them.</p> <p>Lecture 1 Lecture 2 Lecture 3 Lecture 4 Lecture 5</p>"},{"location":"GPU/","title":"GPU","text":"<p>The graphics processing unit, or GPU, has become one of the most important types of computing technology, both for personal and business computing. Designed for parallel processing, the GPU is used in a wide range of applications, including graphics and video rendering. Although they\u2019re best known for their capabilities in gaming, GPUs are becoming more popular for use in creative production and artificial intelligence (AI).</p> <p>GPUs were originally designed to accelerate the rendering of 3D graphics. Over time, they became more flexible and programmable, enhancing their capabilities. This allowed graphics programmers to create more interesting visual effects and realistic scenes with advanced lighting and shadowing techniques. Other developers also began to tap the power of GPUs to dramatically accelerate additional workloads in high performance computing (HPC), deep learning, and more.</p>"},{"location":"Lecture%201/","title":"Lecture 1","text":"<p>Amdahl's Law: The limit on this class Speedup due to enhancement <code>E</code> is  Speedup w/E = Exec time w/o E / Exec time w/E</p> <p>Speedup = 1 / ((1 - F) + F / S) The non speedup part is the left term, and the speedup part is the right term </p> <p>The speedup WITH the enhancement E is the ratio of the execution time without the enhancement divided by the execution time WITH the enhancement. (Derivation is shown below, the <code>T</code> cancels out) Speedup = 1 / [ (1 - F) + (F / S) ]</p> <p>For calculating the total execution time with the enhancement in place what you need to account for is the time that it takes to process the part that is NOT sped up, as well as the time it takes for the part that is sped up to run.  You add these two terms together</p> <p>First, you have <code>T</code> which is the original time that it takes for the entire program to finish So for the <code>slow</code> part, this means it would take T x (1 - F) time. And for the faster part, this means that the total amount of time that would take would be T x (F/S). The S variable introduced is the speedup factor. I think about this as the number of cores being INCREASED overall. So we can divide up the work amongst each CPU core. </p> <p>So for the execution time with the enhancement, this would be  exec. time w/ enhancement = exec. time w/o eh (T) x [(1-F) + F/S] The speedup is the (time before) / (time after). </p> <p>Speedup derivation: T / T(1 - F) + T(F/S) - The T's Cancel out, leaving the speedup to be ---&gt; 1 / (1 - F) + (F / S)</p> <p></p> <p>The speedup that you get from making the program TWICE as fast is the same as making 25 percent of the program 20 times as fast. BOOOOO</p> <p></p> <p>Amdahl's law tells us that in order for us to achieve linear speedup with 100 processors, none of the original computation can be scalar!</p> <p>To get a speedup of 90 from 100 processors, the percentage of the original program that could be scalar would have to be 0.1% or less.</p> <p>We are often limited by the sequential stuff</p> <p></p>"},{"location":"Lecture%201/#strong-scaling-vs-weak-scaling","title":"Strong Scaling vs. Weak Scaling:","text":"<p>Strong Scaling: When speedup can be achieved on a parallel processor without increasing the size of the problem. Weak Scaling:  When speedup can be achieved on a parallel processor by increasing the size of the problem proportionally to the increase in the number of processors.</p> <p>Example: The time it takes to OCR a document - It make take a minute to OCR a doc - You can get really good scaling when you want to OCR 1000 docs, just throw a bunch of processors at it (50 to 100 processors)!     - However, you increased the amount of data as the number of processors increased     - You still have not solved the issue of processing the 1 OCR doc faster</p>"},{"location":"Lecture%201/#another-important-term-load-balancing","title":"Another important term, <code>Load Balancing</code>","text":"<p>Another important factor: every processor doing the same amount of work. - Just one unit with twice the load of others cuts speedup almost in half Example: we have 50 computers to complete one task, the problem of load balancing can arise if we have 1 computer, doing HALF of the work, and the rest split of the other half. No matter how much you try to make it better, it will never get better cause that ONE computer is doing half the work. </p>"},{"location":"Lecture%201/#real-world-examples","title":"Real-world examples \ud83d\ude14","text":"<p>I don't think this will be super important, but here is the slide</p> <p></p>"},{"location":"Lecture%201/#premature-optimization","title":"Premature Optimization","text":"<p>The runtime of a program is really... - The runtime of the program on all the inputs that you will ever run on it - The time it takes you to write the program in the first place So... do NOT Prematurely optimize - If you worry about getting things right first, you may never have to optimize it at all - Additionally, you should worry about making your code readable and well-documented     - This is because the runtime of a modified version of the program is the runtime on all inputs plus the time it takes you to relearn what you did in order to modify it</p>"},{"location":"Lecture%2012/","title":"Latency, Throughput, Dependencies, etc...","text":"<p>The real-world is a distributed, parallel system.</p> <p>We see this often in computing: DNS is a distributed, parallel-access, mostly read-only datastore.</p> <p>The Web is a distributed, parallel-access computational process.</p> <p>Definitions:</p> <p>Latency: The time it takes for the first thingy.</p> <ul> <li>For a data communication network, this would be the time that it takes for the first bit to arrive.</li> <li>For communication, this is often the round trip time. The time it takes a 'ping' message to transmit the network, be responded to, and for it to return. </li> <li>For memory: The time for the for the first bit to return from accessing the memory device</li> <li>For <code>stuff</code>: the time it takes for the first thing to arrive</li> <li>Units of latency are time: Either are directly just seconds, or in another unit of time (clock cycles)</li> </ul>"},{"location":"Lecture%2012/#relating-latency-and-cost","title":"Relating Latency and Cost...","text":"<p>If the capabilities were otherwise the same for two fabrics... you will always and always and should choose the lower latency option.</p> <p>Overall, if you have a choice between low-latency vs. a high latency fabric, you will always choose the low-latency option. Assuming everything else is equal / the same. </p>"},{"location":"Lecture%2012/#relating-the-concept-of-latency-to-memory","title":"Relating the concept of latency to memory","text":"<p>L1 Cache is low latency, however this cache has a low capacity L2/L3 cache are larger and have a higher latency than L1 cache. DRAM is much higher latency</p> <p>DRAM: From a physics standpoint, DRAM is a lot slower to access the first element of memory. But, it is much cheaper (i.e, orders of magnitude cheaper) per bit stored </p> <p>High performance computing is highly memory aware. Most of the focus is centered around ensuring that our memory accesses are fast. It would be amazing if we were able to store everything in L1 Cache.</p>"},{"location":"Lecture%2012/#latency-decision-making","title":"Latency &amp; Decision Making","text":"<p>Your decision cycle (the time it takes to react to something and do the response) has to be longer than the latency itself.</p> <p>If the things happen faster than you are able to react, what are you going to be able to do about it? You can't really do anything because you didn't have enough time to react!</p> <p>Relating this to the real-world: Current shipments arriving left before liberation day So they could only have been affected by the possibility of tariffs not the reality. </p> <p>The whipsawing <code>yes and no</code> cycle on a daily basis can not be handled when the shipping of things takes weeks alone!</p>"},{"location":"Lecture%2012/#bandwidth-throughput-stuff-per-unit-time","title":"Bandwidth / Throughput: Stuff per unit time","text":"<p>Bandwidth/Throughput: The steady state ability to process at a maximum rate of elements.  Units are the amount of thingy per unit time. - Parallel computing is about maximizing the overall throughput on parallel tasks.</p> <p>If you have a sequential task, it might take longer in a parallel computation (i.e, a higher latency) but that's fine. </p> <p>You could want to ship something that will take longer to get to you, but, you are able to get really high-throughput!</p>"},{"location":"Lecture%2012/#improving-memory-bandwidth","title":"Improving Memory Bandwidth","text":"<p>The GPU's secret for performance 1. A lot of memory bandwidth simply by adding more wires     - 900 GB/s max on a 5080 through a combination of wide (256b wide) and fast (very high clock rate) memory busses     - The memory latency is still terrible 2. Doing a lot of work on other things while waiting on memory     - Compute model is focused on being able to stream a massive amount of memory through the device:         - Generate request for a large hunk         - Lots of work on previous hunks         - Get this hunk, generate a request for the next hunk</p>"},{"location":"Lecture%2012/#parallel-throughput-is-an-insufficient-metric-alone","title":"Parallel Throughput is an insufficient metric alone","text":"<p>Throughput alone is not sufficient enough of a metric on a parallel task. If it's parallel, you can always just throw more resources at it! (i.e, real-world examples. Instead of one plane, send 5. Instead of a 64b memory bus, use a 256 memory bus). Sea shipping is so much more attractive because you are moving the same amount of shipment, for a way cheaper cost. This is the way more desirable option. </p>"},{"location":"Lecture%2012/#why-latency-is-than-parallel-throughput","title":"Why latency is &gt;&gt; than parallel throughput","text":"<p>If the task in question is latency bound, then parallel throughput doesn't help. A lot of <code>things</code> are latency bound and not <code>throughput</code> bound. </p> <p>What does it mean for a task to be latency bound? A piece of code is latency bound when the GPU cannot keep busy with the available/exposed parallel work. The general strategy for a latency bound code will be to expose more parallel work</p> <p>If said task is throughput bound, throwing more of a low-latency tool can work. Each unit of work finishes faster leads to more units processed per second.</p> <p>Example: Suppose: You can process 1 file every 100 ms (latency per file = 100 ms). Your throughput = 10 files per second. If you switch to a low-latency tool that cuts latency to 50 ms per file: Throughput now doubles \u2192 20 files per second.</p>"},{"location":"Lecture%2012/#buffering-caching","title":"Buffering &amp; Caching","text":"<p>A flow through device. Often times, processes have a buffer. A location where one can delay/store up to X thingies. EG, a work queue would be a buffer. This is very common on networking to handle contention. If there are multiple inputs wanting to share an output, buffer up some of the traffic. This is a really popular technique to smooth out a fairly bursty process. </p>"},{"location":"Lecture%2012/#caching","title":"Caching","text":"<p>Cache is a store device. Related are caches that can store and quickly access up to X thingies. A small amount of data for a working set. Both the concepts of a buffer and a cache have the notion of capacity: the number of thingies that it can support. </p>"},{"location":"Lecture%2012/#buffering-in-the-real-world","title":"Buffering in the real-world","text":"<p>A business's inventory is a buffer designed to smooth out the bursty process of purchasing. Sell X thingies per day means buying X thingies per day. But buying X thingies is bursty, We get 30X thingies at once. Our solution would be a 30 day buffer inventory. This is very common in parallel services. In the wait for parallelism model, we can often get bursts of requests: a bit of buffering might be useful to smooth out the computation. In parallel services (like web servers, cloud apps, etc.): Requests don\u2019t always arrive evenly. Sometimes, bursts happen \u2192 e.g., lots of user requests at once. Because of the wait-for-parallelism model: Even if the system can process requests in parallel, It might have to wait for the next batch of requests or wait for available workers. Even if a huge number of requests arrive at once, they can wait in the buffer while the workers process them as fast as they can. Weaver discusses the pool of workers architecture. Throw a bunch of workers into a pool, when tasks come off of the buffer, then one of the workers will do something. </p>"},{"location":"Lecture%2012/#measuring-buffering-capacity-and-cost","title":"Measuring Buffering: Capacity and Cost","text":"<p>Buffers and caches have some type of maximum capacity. The number of thingies that can be held in it. You are paying for what the maximum capacity will be, even if you're not using the space for all of it.  The cost is not associated with what you're actually using. (i.e, the utilized capacity). </p>"},{"location":"Lecture%2012/#option-minimizing-capacity","title":"Option: Minimizing capacity","text":"<p>The less capacity, the less cost that you have. Make sure the thingies arrive the day before you need it and not the day after. There are implicit buffers in transit time. High performance DRAM systems do this!</p>"},{"location":"Lecture%2012/#dependencies","title":"Dependencies","text":"<p>A dependency relationship where A depends on B requires that B be completed before A can be initiated. Dependencies will always inhibit parallelism. Broken dependencies cause major issues.</p>"},{"location":"Lecture%202/","title":"Lecture 2","text":"<p>More Philosophy, and Threading What are the two basic approaches to parallelism? - Multiprogramming     - running multiple independent programs in parallel     - Easy - Parallel Computing     - Run ONE program faster     - Hard - In this class, we are mostly focused on the 2nd (Parallel Computing)</p>"},{"location":"Lecture%202/#most-simple-model-sisd-single-instruction-single-data-stream","title":"Most simple model, SISD (Single Instruction, Single Data Stream)","text":"<p>Sequential computer that exploits no parallelism in either the instruction or the data streams Examples of SISD arch. are the traditional uniprocessor machines - RISC-V processor - We consider superscalar as SISD b/c the programming model is sequential - A superscalar processor:  Can fetch, decode, and execute multiple instructions per clock cycle. BUT: all of these instructions come from a single instruction stream (your program\u2019s normal sequence of instructions). - Normal SISD: One chef making one dish at a time. - Superscalar SISD: One chef working on several dishes at once, but still following a single recipe (single instruction stream). SIMD would be like: One chef making several copies of the same dish at once (single instruction, multiple data). MIMD would be: Multiple chefs each following different recipes.</p>"},{"location":"Lecture%202/#single-instruction-multiple-data-stream-simd","title":"Single-Instruction / Multiple-Data Stream (SIMD)","text":"<p>SIMD computer processes multiple data streams using a single instruction stream, e.g, Intel SIMD Instruction extensions or NVIDIA GPU's </p>"},{"location":"Lecture%202/#multiple-instruction-multiple-data-stream-mimd","title":"Multiple Instruction / Multiple-Data Stream (MIMD)","text":"<p>Multiple autonomous processors simultaneously executing different instructions on different data. - MIMD archs. include multicore and warehouse-scale computers - Huge amount of control logic</p> <p></p>"},{"location":"Lecture%202/#problem-types","title":"Problem Types","text":""},{"location":"Lecture%202/#sequential","title":"Sequential","text":"<p>A, then B We have a dependency The dependency is that B depends on A It does not matter how many processors you have, there is a dependency. A lot of Amdahl's law issues come from this Can't do A and B at the same time, cause you have to wait for A to finish before trying to do B</p>"},{"location":"Lecture%202/#simd","title":"SIMD","text":"<p>SIMD is a very constrained type of parallelism For every A, do simple X The key features are that the operation is the same for every element in the group. There is a very limited control flow. - We can do conditional operations - AKA short if conditions - But we usually do X and Y for both and then select the answer:     - Turning into a piece of *straight line cod</p>"},{"location":"Lecture%202/#the-biggest-simd-problem-is","title":"The Biggest SIMD Problem is...","text":"<p>The BIGGEST problem is dense Matrix multiply Image filtering, noise reduction Core operation in Deep learning What does it mean for a matrix to be dense? - This means that most of the entries are non-zero</p>"},{"location":"Lecture%202/#mimd","title":"MIMD","text":"<p>MIMD is far less constrained, for every A, we are gonna do {big complex thing} Can have branches, loops, many different things going on at once - EG, do one thing for A, a different thing for B, and a different thing for C The advantages of MIMD are that it's far more flexible Anything that is SIMD, can be done MIMD! The disadvantage however, is that it is far more expensive MIMD processors are dominated by control logic and SIMD processors are almost completely compute</p> <p>Q: Would a web-server be a MIMD, or a SIMD problem? A: A web-server would be a MIMD problem, cause you have a bunch of web-requests coming from everywhere, from different things. You need flexibility that SIMD just doesn't offer / have the capability to.</p>"},{"location":"Lecture%202/#the-canonical-mimd-problem","title":"The Canonical MIMD Problem","text":""},{"location":"Lecture%202/#sparse-matrix-multiply","title":"Sparse Matrix Multiply","text":"<p>This is the one used to benchmark supercomputers Most entries within the matrix are actually 0 - So far better to skip a whole bunch of multiplications:     - 0 times a row is a row of 0's This is used for bigger simulation problems - Nuclear weapons, weather, etc..</p>"},{"location":"Lecture%202/#gpus-vs-cpus","title":"GPU's vs CPU's!","text":"<p>When a GPU can be used, it is 50x more powerful than a general CPU When the problem works on the GPU, the GPU is vastly faster And when the performance is not limited by moving the problem into the GPU either</p>"},{"location":"Lecture%202/#but-when-does-the-gpu-get-used","title":"But... when does the GPU get used?","text":"<p>The GPU really only gets used for limited applications The CPU gets used all the damn time CPU &gt;&gt; GPU on sequential performance CPU &gt;&gt; GPU on parallel sequential tasks - Since the CPU has 12 cores, this allows 12 seperate sequential tasks to run at full speed</p>"},{"location":"Lecture%202/#it-comes-down-to-branches","title":"It comes down to branches!","text":"<p>SIMD Problem - For each A, do this straight line code MIMD Problem - For each A do this complex branching code The big differentiator is data dependent branches - SIMD cannot handle data dependent branches by going a certain way for A and another way for B</p>"},{"location":"Lecture%202/#throughput-on-its-own-is-meaningless","title":"Throughput on it's own, is meaningless","text":"<p>Throughput is generally defined as: output per unit time Assuming that you have a task that is both parallel and scalable You can always throw more resources at it and make it run faster</p> <p>EG, I can get the same throughput on a SIMD task with  - 1 GPU  - 500 CPUs We care about throughput / unit-cost!! - W, \\(cm^3\\), $ (moneyyy)</p>"},{"location":"Lecture%202/#performance-vs-efficiency-cores","title":"Performance VS. Efficiency Cores","text":"<p>For a modern CPU, it generally has multiple performance oriented processor cores - The main goals of these types of cores is to run a single stream of execution as fast as possible! - Some processors may support two streams of execution within a single core     - i.e, AMD and Intel</p> <p>CPU's also have multiple efficiency cores     - These run slower but they have a simpler design as well as a slower clock rate. The energy used per operation is vastly slower.</p>"},{"location":"Lecture%202/#wait-parallelism-can-save-energy","title":"Wait, parallelism can save energy?","text":"<p>Yes, parallelism can help save energy! If the parallel version performs the same total operations, running across many efficient cores can consume less power than running the sequential version on a single high-performance core. This is because lower clock speeds and more efficient cores reduce energy per operation. That\u2019s why throughput per unit X (i.e, joule) is a better metric than just raw speed.</p> <p>Parallelism doesn\u2019t just improve speed, it can also reduce energy usage by spreading work across efficient, lower-power resources. We can use half the energy than usual.</p>"},{"location":"Lecture%202/#programs-and-threads","title":"Programs and Threads...","text":"<p>A program is a single unified memory space If you don't explicitly set up shared memory, each program has its own address space. We are able to run multiple copies of the same program. However, these copies do not share any of the data memory. If you do a write in one instance, that is not seen in any other instance (i.e, copy)</p>"},{"location":"Lecture%202/#whats-within-a-program","title":"What's within a program","text":"<ul> <li>At least one thread of execution</li> <li>A program counter: where in the program the current thread is</li> <li>The call stack: Local variables for the current thread's execution  </li> </ul> <p>Every single thread has it's own unique call-stack. They can read data out of each of there call stacks. </p>"},{"location":"Lecture%202/#different-types-of-threads","title":"Different types of threads","text":"<ul> <li>Hardware Thread: What the CPU itself supports<ul> <li>Able to run one (or sometimes two), hardware threads per CPU core</li> <li>This is the number of tasks that the CPU can run at the same time</li> </ul> </li> <li>Operating System (OS) thread: What the OS schedules<ul> <li>The OS is responsible for mapping a (i.e, maybe even unlimited) number of OS threads, onto the running hardware</li> </ul> </li> <li>Software Thread: What the program thinks of...<ul> <li>Usually maps 1-1 with OS threads, but, we will see that we can have MANY more software threads than OS threads (i.e, software threads would be co-routines)</li> </ul> </li> </ul>"},{"location":"Lecture%202/#what-is-the-use-for-threads","title":"What is the use for threads?","text":"<p>Increasing performance by operating in parallel - Allows us to take a lot of advantage of the multiple processor cores - Ideally, you want the same number of threads as you have hardware threads Increasing performance by avoiding waiting - Do stuff while waiting for other stuff</p>"},{"location":"Lecture%202/#what-the-hell-is-the-problem-with-threads","title":"What the hell is the problem with threads?","text":"<p>If two different threads are working on different things, there is no problem at all. If we have two threads that are reading the same thing, still, there is no problem However, here is where shit can go wrong, if one of the threads writes things     We can get something called a race condition  The way that weaver describes it, is that a program seems to not be deterministic Two memory access form a data race if different threads attempts to access the same location, and at least one is a write, and they occur one after another If there is a data race, the result of the program vary's heavily and depends on chance We avoid data races by synchronizing writing and reading so that we can get deterministic behavior Synchronization done by user-level routines that rely on hardware synchronization instructions</p>"},{"location":"Lecture%202/#analogy-buying-beer-in-the-after-times","title":"Analogy: Buying Beer in the after times...","text":"<p>Your fridge has no beer. You and your roommate will return from classes at some point and check the fridge</p> <p>Whoever gets home first will check the fridge, go and buy beer, and return</p> <p>What if the other person gets back while the first person is buying beer? Dude, you just bought TWICE the amount of beer that you need! UGH!</p> <p>It would have been helpful if someone left a note that they went to go buy more beer...</p>"},{"location":"Lecture%203/","title":"Lecture 3","text":""},{"location":"Lecture%203/#threads-synchronization-and-coroutines","title":"Threads, Synchronization, and coroutines","text":"<p>Another real-world example of locks Let's say you are working with a piece of dangerous piece of equipment, something that IF it turns on, you will die horribly! - Power off the system - Lock the power off - Add an ownership tag for who turned this shit off If you want to unlock it, you do everything to find the owner This relates directly to critical sections</p>"},{"location":"Lecture%203/#the-simplest-type-of-lock-the-lock-mutex","title":"The simplest type of lock: The Lock / Mutex","text":"<p>Effectively every single programming language with concurrency has a lock or a mutex. Only one thread at a time can <code>own</code> the lock There are two primary operations when it comes to working with locks - Lock: Acquire the lock. If the lock is not available, block until it is available - Unlock: Release the lock, unblocking another thread if it is appropriate Often also a <code>try-lock</code> - Returns a boolean: True if it was unlocked (Grabs the lock), false otherwise. This is usually not advised to do however. </p>"},{"location":"Lecture%203/#weavers-opinion-of-what-the-best-type-of-lock-is","title":"Weaver's opinion of what the best type of lock is...","text":""},{"location":"Lecture%203/#cs-stdunique_lock","title":"C++'s std::unique_lock","text":"<p><pre><code>std::mutex m;\n...\n{ std::unique_lock ul(m);\n\n}\n</code></pre> - In reality, we could just manipulate the mutex directly     - Three operations of mutexes: lock, try_lock, and unlock - C++ has some nice RAII semantics! - unique_lock is a wrapper around a mutex, and when the unique_lock acquires the lock, it will inherently lock the mutex - The cool part about unique_locks are that when it goes out of scope, unique_lock will release it! (Unlocking the mutex automatically!) - You don't have to worry about unlocking it manually</p>"},{"location":"Lecture%203/#raii","title":"RAII","text":"<ul> <li>C++ and Rust memory semantics know when an object is unreachable<ul> <li>Goes out of scope in a function</li> <li>Reference counted smart pointer decrements all the way down to <code>0</code></li> <li>Unique pointer is unreachable</li> <li>Explicit delete</li> </ul> </li> <li>Objects have the destructor called promptly</li> </ul>"},{"location":"Lecture%203/#features-of-cs-stdunique_lock","title":"Features of C++'s std::unique_lock","text":"<ul> <li>It has move, but NOT copy semantics<ul> <li>What this means is that we can return it's value, but we cannot pass around the value</li> </ul> </li> <li>It acts as an RAII wrapper, so you never need to unlock the lock</li> <li>Instead, the lock just goes out of scope, and it goes 'poof, unlock' IF it was locked</li> <li>We are still able to pass it as an argument via reference</li> <li>We also still have the ability to lock and unlock it manually</li> </ul>"},{"location":"Lecture%203/#but-what-about-little-old-rust","title":"But, what about little old Rust?","text":"<ul> <li>There's no unlock function at all</li> <li>The lock can only be unlocked automatically, we don't even have the option of doing it manually</li> <li>Rust's locks have the notion of being poisoned, what does this mean?</li> <li>Rust allows individual threads to panic. </li> <li>When a thread panics, we assume that the specific data it was operating on may be in a corrupt or invalid state, especially if the panic occurred while modifying shared data.</li> <li>We assume that the data is in an inconsistent state</li> </ul>"},{"location":"Lecture%203/#other-language-locks","title":"Other language Locks:","text":""},{"location":"Lecture%203/#openmp-critical","title":"OpenMP critical...","text":"<p>An extension to C++, working with <code>pragmas</code></p> <pre><code>#pragma omp critical\n{\n    ...\n}\n</code></pre> <p>This designates the region of code as 'critical' An implicit single lock is added for this particular region  Starting the block, will acquire the lock Ending the block, releases the lock</p> <p>This gives us the ability to say that only one thread can enter this region at a time</p>"},{"location":"Lecture%203/#javas-synchronized","title":"Java's synchronized","text":"<p>Java is one of the languages that allows locks to be reentrant What this means is that the same thread can acquire the same lock multiple times without deadlocking itself. Every time the thread locks it, increments an internal counter.  The thread must unlock the lock the same number of times before the lock is released for others.</p>"},{"location":"Lecture%203/#gos-rwmutex-lock","title":"Go's RWMutex ('Lock')","text":"<pre><code>var l sync.RWMutex\n</code></pre> <p>There are two different versions of locking: <code>RLock</code> and <code>Lock</code></p> <p>As many can call RLock as desired. This is because multiple threads can read the same data. This does not cause any type of issues, we just have multiple readers.</p> <p>However, when a call to Lock happens, no further RLocks will succeed. Until the Lock call gets the lock and then releases it: - prevents starvation when there are a lot of readers</p>"},{"location":"Lecture%203/#locks-as-ordering-barriers","title":"Locks as ordering barriers","text":"<p>Within a single thread, writes happen 'in order' Our code could have the following: a = x; y = a;</p> <p>The write to a happens before the write to y</p> <p>But, this same visibility does not apply to other threads - Another thread may see the write to y (of what a becomes) before the write to a is seen! Locks or any other type of synchronization primitive also act as ordering barriers Ensuring that all writes will complete before the other thread can see the data.</p>"},{"location":"Lecture%203/#locks-have-limitations","title":"Locks have limitations","text":"<p>It isn't the most flexible of synchronization primitives... This is because the nature of how they work is that it is only 'one at a time', and everything else is locked out When a lock is released another thread block on the lock resumes We want much more than that, we want the ability to signal as well Two options for higher-level operations: - Condition Variables - Work Queues / Channels</p>"},{"location":"Lecture%203/#synchronization-variables","title":"Synchronization Variables","text":"<p>In addition to the lock, we have a synchronization variable, that is associated with it. Two operations that are supported: 1. wait: in an atomic operation it releases the lock and sets this current thread to be in the waiting state 2. notify: take one or all of the waiting threads and notify them</p>"},{"location":"Lecture%203/#what-about-when-we-wake-up","title":"What about when we wake up?","text":"<p>Thread:     Acquire lock     Change shared data     (optional) Notify other threads while holding the lock     Release lock     (optional) Notify other threads after releasing the lock</p>"},{"location":"Lecture%203/#cs-stdcondition_variable","title":"C++'s std::condition_variable","text":"<pre><code>std::condition_variable::wait\n- void wait( std::unique_lock&lt;std::mutex&gt;&amp; lock );\n- void wait( std::unique_lock&lt;std::mutex&gt;&amp; lock, Predicate pred );\n</code></pre> <p>Wait has two options... - The first wants just a unique_lock - The second wants a predicate function as well - On waking up, the 2nd version evaluates the predicate, and, if false, calls wait again - Predicate is usually best done as a lambda function</p> <pre><code>[&amp;]{return statement; }\n</code></pre> <p>Threads have to communicate with each other. This will be in regards to some resource as well, a DB, a particular part of the physical memory, etc.</p> <p>We want to have overall program control. We cannot / should never have non-deterministic behavior within our program.  Recall, when one of the threads tries to do a write, then issues arise. Just one of the threads has to do a write in order for there to be an issue. </p> <p>Thread 1 could be trying to do a read, and then, a write to a variable, however, the little gap of time between the read and the write, another thread is able to come in and read in a stale value of the variable that we were reading. (Not good) </p> <p>The middle part is called the critical section. Coordination between threads, get a consistent behavior of our overall program. Mutual exclusiveness.</p> <p>When a <code>.wait()</code> takes place, you do not need to stick around, you can just go to another thread and do something there. (Flipping your core around) get busy, quit being lazy.</p>"},{"location":"Lecture%203/#the-producer-and-the-consumer-problem","title":"The Producer and the Consumer problem","text":"<p>This is a very common problem. We have a bunch of producers that generate data/requests. (This can be one / or more threads)</p> <p>We have a bunch of consumers that accept data / requests (again, this can be one, or more threads)</p> <p>So, what we need, and want, is a queue!</p>"},{"location":"Lecture%203/#queue-properties","title":"Queue Properties","text":"<p>Capacity: Queue can hold 0, n, or infinite elements with each option specified.</p>"},{"location":"Lecture%203/#how-does-blocking-work-when-it-comes-to-the-producer-and-consumer-problem","title":"How does blocking work when it comes to the producer and consumer problem?","text":"<p>If there is no data available, a consumer will block until data is available in the queue</p> <p>If there is no space in the queue, a producer will block unitl there is space that is available</p>"},{"location":"Lecture%203/#special-case-of-a-size-0-work-queue","title":"Special case of a size '0' work queue","text":"<p>Blocks until both a producer and a consumer are available and then the queue opens when they are. </p> <p>(Also referred to as a work queue)</p>"},{"location":"Lecture%203/#the-expense-of-os-threads","title":"The Expense of OS Threads","text":"<ul> <li>Memory<ul> <li>An OS Thread neeeds enough space for a large call stack (about 8 MB per thread or so)</li> <li>Has to be a contiguous allocation of memory</li> </ul> </li> <li>Startup<ul> <li>Starting an OS thread requires invoking the OS through the interrupt handler</li> </ul> </li> <li>Switching<ul> <li>Switching between threads involves the OS through the interrupt handler. (Context switching, round robin, time-slicing, etc)</li> </ul> </li> </ul> <p>Note: What is theinterrupt handler?</p> <p>Another problem with OS threads are that they are expensive!</p>"},{"location":"Lecture%204/","title":"Lecture 4","text":""},{"location":"Lecture%204/#implementing-a-work-queue-in-c","title":"Implementing a <code>work queue</code> in C++","text":"<pre><code>// Preliminaries\n#include &lt;queue&gt;\n#include &lt;mutex&gt;\n\ntemplate &lt;class T&gt;\nclass WorkQueue\n{\nprivate:\npublic:\n    WorkQueue() {};\n    // It doesn't make sense to hold 0 elements, and \n    // we use 0 capacity to be \"unlimited capacity\"\n    WorkQueue(size_t size)\n    {\n        capacity = size;\n        asser(size &gt; 0);\n    }\n}\n</code></pre> <p>Fixed capacity (of at least 1) or unbounded IF queue if full, pushing will block until it is not full IF queue is empty, popping will block until it is non-empty Minimum wake ups necessary - Only send a wakeup if there may be something waiting - Only send a single wakeup - \"Unlock than wake\" paradigm</p> <pre><code>...\nWorkQueue(const WorkQueue &amp;) = delete;\nvoid operator=(const WorkQueue &amp;) = delete;\n\nprivate:\n    std::queue&lt;T&gt; data;\n    std::mutex lock;\n    std::condition_variable notify_get;\n    std::condition_variable notify_put;\n    size_t capacity = 0;\n</code></pre> <p>It makes absolutely no sense to make a work queue copy-able / mov-able.</p>"},{"location":"Lecture%204/#logic-on-safety-regarding-this-workqueue-implementation","title":"Logic on safety regarding this workQueue implementation","text":"<p>We ensure that whenever we are manipulating the queue, that we have a lock - If the queue is empty there may be something waiting to read - If the queue is full there may be something waiting to write - It is safe to send a spurious wakeup if nothing is waiting</p> <p>What we want to do is avoid the wait and lock motif Thus, we only signal on the condition variable after we unlock</p>"},{"location":"Lecture%204/#putting-elements","title":"Putting elements...","text":"<pre><code>void put(T &amp;element)\n{\n    bool wasempty = false;\n    // Paranthesis are important for the 'scope' of what we\n    // are doing.\n    {\n        std::unique_lock l(lock);\n        while (capacity != 0 &amp;&amp; data.size() &gt;= capacity)\n        {\n            notify_put.wait(1);\n        }\n        wasempty = data.empty();\n        data.push(element);\n    }\n    if (wasempty)\n        notify_get.notify_one();\n}\n</code></pre>"},{"location":"Lecture%204/#getting-elements","title":"Getting elements","text":"<pre><code>T get()\n{\n    bool wasfull = false;\n    std::unique_lock&lt;std::mutex&gt; l(lock);\n\n    // This while-loop right here is critical\n    while (data.empty()) { \n        notify_get.wait(l); \n    }\n    if (data.size() &gt;= capacity) { \n        wasfull = true; \n    }\n\n    auto ret = data.front();\n    data.pop();\n    l.unlock(); // Explicit unlock happening\n    // Another thread could sneak in here, and it \n    // would not be a problem, this is due to the fact\n    // that we have our while \n    // loop above re-checking the condition\n    if (wasfull) {\n        notify_put.notify_one();\n    }\n\n    return ret;\n}\n</code></pre>"},{"location":"Lecture%204/#what-if-a-writer-sneaks-in","title":"What if a writer sneaks in?","text":"<p>Let's suppose that a writer sneaks in between a pop() and the notify? - That one will sneak in and be able to do a <code>push()</code> - But the other one will just go to sleep and go back to waiting... - And the next pop() will go \"Hey, things are full, I need to notify\" - Have to go through such logic to make sure orderings are good</p> <p>The rest of the lecture of Weaver going through coding examples - Setting breakpoints, and the importance of randomization when testing</p>"},{"location":"Lecture%204/#the-expense-of-os-threads","title":"The Expense of OS Threads","text":"<ul> <li>Memory<ul> <li>An OS Thread neeeds enough space for a large call stack (about 8 MB per thread or so)</li> <li>Has to be a contiguous allocation of memory</li> </ul> </li> <li>Startup<ul> <li>Starting an OS thread requires invoking the OS through the interrupt handler</li> </ul> </li> <li>Switching<ul> <li>Switching between threads involves the OS through the interrupt handler. (Context switching, round robin, time-slicing, etc)</li> </ul> </li> </ul> <p>Note: What is theinterrupt handler?</p> <p>Another problem with OS threads are that they are expensive!</p> <p>(Recap from the previous lecture where he left off)</p> <p></p> <p>The solution is Coroutines. which exist in Kotlin and Go Within Go, every function is potentially suspending The user level allocates 1 OS thread for each CPU Then manually schedules and switches between user threads</p>"},{"location":"Lecture%204/#how-are-go-routines-co-routines-so-cheap","title":"How are go routines (co-routines) so cheap?","text":"<p>Go routines are cheap! They do not use the conventional C Call stack It uses a set of linked-list connected hunks</p> <p>Allocates a 4 kilobyte hunk. It does dynamic allocation of growth for the call stack for which is necessary. The call stack for go is NOT contiguous, but is allocated with these disconnected hunks.</p> <p></p>"},{"location":"Lecture%204/#launching-a-go-routine","title":"Launching a go routine","text":"<pre><code>go function(args...)\n</code></pre> <p>This is the syntax to spawn a new go-routine. This is just like a normal function call, but with the word go in front of the function. It evaluates all of the arguments in the current go routine. And then, it launches the function itself in a new go-routine. What it really is, is call this function in a new coroutine, and ignore the return value.  Go does not have a conventional join for threads.</p>"},{"location":"Lecture%204/#remember","title":"Remember...","text":"<ul> <li>In go, every function is potentially suspending</li> <li>Which gives far more opportunities for the go scheduler to switch between currently running tasks</li> <li>There is s still a timer interrupt</li> <li>The scheduler will receive a timer interrupt on a regular basis to execute a mandatory context switch</li> </ul>"},{"location":"Lecture%205/","title":"Channels in Go","text":"<ul> <li>A channel is essentially a work queue with a specified capacity</li> </ul> <pre><code>c := make(chan string)\n// A channel for strings, with the default capacity of 0\n</code></pre> <p>When a channel in go has a buffer capacity of some sort, this means that they work like a FIFO queue.</p> <p>Q: When do the sender and the reciever block when reading and writing from a channel? A:</p> <p>Q: How come channels with no buffer in Go are special? A:</p>"},{"location":"Lecture%205/#channels-alone-are-not-enough","title":"Channels alone are not enough","text":"<p>A channel is just a work queue, this is not enough, what MAKES a channel special is how it works with the select statement. This is a way to wait on multiple / consecutive channel operations</p> <pre><code>// Select statements working with Channels in Go\nselect {\n    clause:\n        stmt+\n        ...\n}\n</code></pre> <p>Q: What does the clause have to be? A: The clause has to be a channel operation <pre><code>c &lt;- \"foo\"\nx:= &lt;- c\n\n// 1. Placing the string \"foo\" onto the channel (i.e, channel write)\n// 2. declaring a variable named x, and reading from the channel `c`\n</code></pre></p> <p>The part to the right of the channel operation is evaluated. Then if one channel operation can complete, it is run - Otherwise, things block until a channel is available Then, when one channel operation can complete, it is run</p>"},{"location":"Lecture%205/#go-by-example","title":"Go, by example","text":"<pre><code>func main() {\n    c1 := make(chan string)\n    c2 := make(chan string)\n    go func() {\n        time.Sleep(1 * time.Second)\n        c1 &lt;- \"first\"\n    }()\n    go func() {\n        time.sleep(2 * time.Second)\n        c2 &lt;- \"second\"\n    }()\n    for i:= 0; i &lt; 2; i++ {\n        select {\n        case msg1 := &lt;-c1:\n            fmt.Println(\"got message for\", msg1)\n        case msg2 := &lt;-c2:\n            fmt.Println(\"got message for\", msg2)\n        }\n    }\n\n}\n</code></pre> <p>Q: Which select statement will run first? A: I believe that c1 goes first because we launched that go function first.</p> <pre><code>\n</code></pre>"},{"location":"Lecture%207/","title":"Lecture 7","text":""},{"location":"Lecture%207/#deadlock","title":"Deadlock","text":"<p>Definition: a system state in which no progress is possible because everything is locked waiting for something else.</p>"},{"location":"Lecture%207/#dining-lawyers-problem","title":"Dining Lawyer's Problem:","text":"<ul> <li>Pontificate until the left fork is available, when it is, pick it up</li> <li>Pontificate until the right fork is avail, when it is, pick it up</li> <li>When ya got both of the forks, then start eating big boy</li> <li>But you can only eat for a certain amount of time</li> <li>Alright, you are done eating big boy, let's let another big boy eat</li> <li>Put right fork down</li> <li>Put left fork down</li> <li>Repeat from the beginning</li> </ul>"},{"location":"Lecture%207/#different-classes-of-deadlocks","title":"Different classes of deadlocks","text":"<ul> <li>Architectural Maldesign<ul> <li>The dining lawyers</li> </ul> </li> <li>No counterparty</li> <li>Recursive locking</li> <li>Indirect recursive locking</li> </ul> <p>The issue with the dining lawyers is that just one fork grab is atomic, but, both fork grabs are not atomic. A lawyer will hold onto a fork and cannot steal them from each other. The order they attempt to acquire the forks creates a cycle: </p> <p>Everyone can acquire lock A and nobody can get lock B.</p> <p>Our solution to this is to change the overall architecture of our design, to prevent this.</p>"},{"location":"Lecture%207/#techniques-of-solving-the-deadlock-problem","title":"Techniques of solving the deadlock problem","text":""},{"location":"Lecture%207/#method-1-of-fixing-a-deadlock-global-lock","title":"Method #1 of fixing a deadlock: global lock","text":"<p>Instead of having the sequential process of grabbing lock A, and then grabbing lock B. What we do is have there be a global lock, and we grab the global lock.  This allows to have the atomic operation of grabbing both forks. Each lawyer just grabs the global lock, which is equivalent to grabbing 2 locks sequentially, this gets rid of the deadlock problem. Only atomic operations are occurring. Creating this global lock thingy is not good, because this fucks our throughput over. Things get slow as shit.</p>"},{"location":"Lecture%207/#method-2-of-fixing-a-deadlock-grab-even-first","title":"Method #2 of fixing a deadlock: Grab even first","text":"<p>Number the forks 1-n - Instead of the lawyer grabbing the 'letf' fork first - Grab the 'even' fork first Now, there will always be a spare fork for somebody - Specifically, starting with the last one What this does is break the cycle, b/c deadlocks only occur when there is a cycle of locks. What we need to do is ensure that this cycle can never occur.</p>"},{"location":"Lecture%207/#method-3-of-fixing-deadlock-just-kill-one","title":"Method #3 of fixing deadlock: Just Kill One...","text":"<p>Detect that no forward progress is being made... - Everyone has a fork but no one is eating Pick a lawyer, and kill that dude \ud83d\ude2c Now, we should be able to make forward progress! A variant of this solution is that we just never let it happen.  (i.e, only allowing 'n-1' lawyers, so an extra fork between one pair of lawyers)</p>"},{"location":"Lecture%207/#the-common-deadlock-bug-no-counterparty","title":"The common deadlock bug: No counterparty...","text":"<p>In the Go PL, this is more common for channels, we can have coroutine 1, which is wanting to write / read to a channel. However, there is no other co-routine that knows about the channel. This is guaranteed to deadlock. Go will just panic this since the Go runtime will have knowledge that no other coroutine knows about the channel we are reading and writing to.</p>"},{"location":"Lecture%207/#the-common-deadlock-bug-direct-recursive-lock-channel","title":"The common deadlock bug: Direct Recursive Lock / Channel","text":""},{"location":"Lecture%207/#a-more-subtle-deadlock-bug-indirect-recursion","title":"A more subtle deadlock bug: Indirect Recursion...","text":""},{"location":"Lecture%207/#race-conditions","title":"Race conditions","text":"<p>Two threads accessing the same data, but, one of them is writing</p>"},{"location":"Lecture%207/#dns-overview","title":"DNS Overview","text":"<p>DNS will turn a human abstraction (www.google.com) into an IP Address Can also contain other data. Known as a performance-critical distributed database.</p> <p>DNS security is essential for the web. DNS is based on a notion of hiearchical trust. (You trust . for everything, .com. for everything .com, so on and so forth)</p>"},{"location":"Lecture%207/#dns-lookups-via-a-resolver","title":"DNS Lookups via a Resolver","text":"<p>We are able to cache entries for a certain period of time</p>"},{"location":"Lecture%207/#dns-protocol","title":"DNS Protocol","text":"<p>A lightweight exchange of query and reply messages, both with the same message format. DNS is primarily through UDP, if you use TCP, this just adds a 2 byte message length. All servers are on port 53. Clients usually use port 53, but, they can use any port.</p> <p></p> <p></p>"},{"location":"Lecture%207/#dns-records-and-rrsets","title":"DNS Records and RRSETs","text":"<p>DNS resource records (RR) can be of many types - Name TYPE Value     - Also a TTL field, how long in seconds this entry can be cached for - Addressing     - A: IPv4 Addresses     - AAAA: IPv6 Addresses     - CNAME: Aliases, 'Name X should be name Z'     - MX: 'The mail server for this name is B' - DNS related:     - NS: 'The authority server you should contact is named Y'     - SOA: \"The operator of this domain is Y\" - Other:     - Text records, cryptographic info - Groups of records form an RRSET     - i.e, all the nameservers for a given domain</p>"},{"location":"Lecture%208/","title":"Lecture 8","text":""},{"location":"Lecture%208/#caches","title":"Caches ($)","text":"<p>Computer caches are critical for modern performance. This is because accessing main memory can take hundreds of instructions worth of time.  Our computers have relatively small caches, they are just small memories that shadow a larger part of bigger memory. (i.e, RAM)</p> <p>Caches are usually arranged into blocks. (64B or 128B). Memory transfers usually take place around these block sizes. Caches are critical for maintaining performance. </p>"},{"location":"Lecture%208/#caches-locality-temporal-vs-spatial","title":"Caches &amp; Locality (Temporal vs. Spatial)","text":"<p>Temporal Locality: If you use data at location X now, you will likely use it  Spatial Locality: If you use data at location X now, you will likely use it at X + 1 soon.</p> <p>The goal is to take advantage of both temporal and spatial locality. If it is able to, it's a hit, if not, it's a miss.</p>"},{"location":"Lecture%208/#cache-miss-types","title":"Cache Miss Types","text":"<p>Compulsory: This first type of cache miss must occur because the data has not been accessed ever before yet. A compulsory miss can be reduced by pre-fetching data.</p> <p>Capacity: The cache miss occurs because the data was evicted because there was too much stuff in the cache</p> <p>Conflict: The cache miss occurred because the data was evicted because there was sharing in the cache: Some other data needed to be put in the same place. </p> <p>We cannot avoid compulsory misses completely. We can minimize them though by prefetching data of course, so that we do not have to go to main memory as often. </p> <p>Conflict misses are pretty rare. - This rarely happens because most modern processors have very highly associative caches:  - 16 way is quite common meaning that you would have 17 different pieces of data mapping the same location - </p> <p>Capacity misses are the biggest problem, what we need to do is work on smaller pieces of data. </p> <p>A really important fact of optimization is that the sequential part of our program runs really fast / efficiently. Otherwise, there really isn't a point.</p>"},{"location":"Lecture%208/#array-strides","title":"Array Strides","text":"<p>Key things to know: sizeof(int) = 4 bytes array size = 32 elements Fully assoc. cache line size = 16 bytes number of lines = 16</p> <pre><code>int sum_array(int *my_array, int size, int stride)\n{\n    int sum = 0;\n    for(int i = 0; i &lt; size; i+= stride)\n    {\n        sum += my_array[i];\n    }\n}\n</code></pre> <p>Let's look at two cases, when the stride is value of 1, and when the stride is  a value of 2.</p>"},{"location":"Lecture%208/#when-the-stride-length-is-1","title":"When the stride length is 1","text":"<p>Initially, you have a compulsory miss, after the compulsory miss, you then, will bring in (pre-fetch) data from main memory. Filling it up all the way to your line size (16 bytes). After doing so, you will keep getting cache hits until you reach the end of your line, where, you will have to go to main memory, and grab in data again. Visually, here is what that looks like.</p> <p>We have a good hit / miss ratio because our stride is just <code>1</code>. So we are always looking at adjacent elements. </p> <p></p> <p></p>"},{"location":"Lecture%208/#when-the-stride-length-is-2","title":"When the stride length is 2","text":"<p>Now, here is where issues start to arise</p> <p>Previously, when the stride length was just 1, we were bringing data into our cache line, and we were using it pretty often, (As noted by the number of hits). However, what if I simply wanted to sum our array up based on every other value? (Every 2nd value)</p> <p>We introduce a new term called <code>'brought in, but unused'</code>, let's take a look</p> <p></p>"},{"location":"Lecture%208/#stride-length-of-4-not-good","title":"Stride length of 4, not good!","text":"<p>If our stride is &gt;= block length, we do not have an advantage of bringing the entire line in.</p>"},{"location":"Lecture%208/#matrix-multiply","title":"Matrix Multiply","text":"<p>Everyone times everyone, and boom, you add up to there! (Noted with the orange colors). Unfortunately, matrix multiply is a very very very common problem in the real-world (i.e, lots of applications)</p> <p>When you multiply two matrices together, the end result is the product.  Multiplying the two rows, gets you this one square in the last matrix (i.e, the product) </p> <p>Note: Arrays are commonly stored in row-major order.</p> <p>When we do the standard matrix multiply, we make really bad use of our cache. </p> <p>The reason that we get a bunch of cache misses within the B matrix, is b/c of the architecture that we are storing our data in. We store things in row-major order. This means that we are essentially always going to be going to main memory when we try to get values for this matrix. (Depending on the arch. matrices can be stored in column major order as well) When we get to the next column, what we suffer from is something called a cache eviction, so we can't even reuse the things that were stored previously in the cache. We keep having to go to main memory, which is of course not something that we want to do. What we need to do is transpose our matrix to solve this problem of constant cache evictions.</p> <p>Thus, what this motivates us to do, is to transpose matrix B, and ensure we are utilizing our cache effectively. </p> <p>When you transpose matrix B, what this ends up doing is reduce your working-set size. The technique of cache blocking comes in b/c instead of working with this large set of data, let's just work with smaller pieces of data to be more effective with how we are storing things in the cache. </p>"},{"location":"Lecture%208/#cache-blocking","title":"Cache Blocking","text":"<p>A technique where data accesses are rearranged to make better use of the data that is brought into the cache. Helps prevent repeatedly evicting and fetching the same data from the main memory. </p> <p>The data access being <code>rearranged</code> in this case would be just accessing those little blocks and such instead of the entire row. </p> <p>Visual representation: </p> <p>As a result, what he have is a really low cache miss rate. We are not getting as many cache misses as we would have without using the cache blocking technique.  Find other articles on the concept of cache blocking</p> <p>Although this is sequential, this is freaking critical for sequential performance. If your cache is cooked, then you are not going to ever get good parallel performance. You will always just be waiting on main memory.</p>"},{"location":"Lecture%208/#loop-un-rolling","title":"Loop Un-rolling","text":"<p>Compiler writers spend a huge amount of time spent optimizing loops. This is due to Amdahl's law. The inner loop of the program is where you spend the most amount of your time in. The caveat is that compilers are often optimized around relatively simple loop shapes. So nothing super complicated. (The bounds should be easy to compute and are small in general) Loop unrolling works really well with good caches. </p> <p><pre><code>for(auto i = 0; i &lt; 4; ++i)\n{\n    ...\n}\n</code></pre> A loop with small and fixed bounds.  You gotta make sure that you have the optimizer on! <code>(O2, O3, etc)</code></p> <pre><code>for(auto j = 0; j &lt; (max &amp; ~3); j = 4){\n    for(auto i = 0; i &lt; 4; ++i){\n        ar[i + j] += src[i + j] * src2[i + j];\n    }\n\n}\n</code></pre> <p>With the optimization flag, what the compiler does is turn the simple loop above, into this: <pre><code>for(auto j = 0; j &lt; (max &amp; ~3); j += 4)\n{\n    ar[j] += src[j] * src2[j];\n    // this is unrolled 4 more times\n    ar[j + 1] ... \n    ar[j + 2] ...\n    // Continues, all the way to `j + 3`\n}\n</code></pre></p> <p>Gonna have to review concepts regarding loop unrolling</p>"},{"location":"Lecture%208/#chip-multicore-multiprocessor","title":"(Chip) Multicore Multiprocessor","text":"<p>SMP: (Shared Memory) Symmetric Multiprocessor - Two or more CPUs/Cores - Single shared coherent memory </p> <p>Single physical address space shared by all of the processors / cores Processors coordinate / communicate through shared variables in memory (via loads and stores)  - Use of shared data has to be coordinated via synchronization primitives (i.e, locks) that allow access to data only one processor at a time. Basically all multicore computers today are SMP</p>"},{"location":"Lecture%208/#multiprocessor-caches","title":"Multiprocessor Caches:","text":"<ul> <li>Memory will always be a bottle neck, even within a uni-processor system</li> <li>We use caches to reduce the bandwidth demands to main memory</li> <li>Each core within a multicore system has their own private cache.</li> <li>Only cache misses will have to access to the common shared memory</li> </ul> <p>The problem amongst multiprocessor caches: Coherency issues</p> <p>Two CPU's on a bus both read the same data; each processor gets a copy of the data and stored it within their respective cache. Then, one CPU performed a write, their cache is up to date, but the other processor has stale data (it is unaware of its own data being stale) </p> <p>How can we communicate when one processor changes the state of the shared data? Doers every processor action cause data to change state? Who should be responsible for providing the updated data? What happens to memory while all of this is happening?</p>"},{"location":"Lecture%208/#what-is-the-goal-of-cache-coherence","title":"What is the goal of cache coherence?","text":"<p>Architect's job: shared memory -&gt; keeping cache values coherent</p> <p>When any processor has cache miss or writes, notify other processors via the interconnection network.  - If you are only reading, many processors can have copies - If a processor writes, invalidate any of the other copies</p> <p>Write transactions from one processor, other caches \"snoop\" the common interconnect checking for tags that they hold. - We invalidate any copies of same address modified in the other cache</p> <p>Keeping state &amp; MSI: Just like we have the <code>valid</code> and <code>dirty</code> bits for each corresponding cache block, we now introduce a new bit called <code>shared</code>. Remember, state is kept at a block by block basis. All of the data within a block should hypothetically have the same state. </p>"},{"location":"Lecture%208/#more-enhancements","title":"More enhancements:","text":"<p>We want to use write-back caches. A write-through cache design uses much more memory bandwidth. We want to minimize the number of writes overall. So \"write-back\" even in the case of shared cache blocks!!!</p> <p>We can communicate by broadcasting requests (i.e, shout out to all of the other processors). What is absolutely insane is that even getting some piece of information from a neighboring cache is still much faster / quicker than trying to access something from main memory. Look in your friend's cache! Fuck it. </p> <p>What is the difference between a write-back cache and a write-through cache? Write back will just update the cache, and mark the block as dirty. The main memory will later get updated, when that block is replaced or explicitly flushed. Write-through: Every time that the CPU writes data to the cache,  it will also write the same data to the main memory immediately. </p> <p>Now, each cache tracks the state of each block in the cache: 1. Shared: up-to-date data, other caches may have a copy. (Valid bit is set, and the shared bit is set for this block) 2. Modified: up-to-date data, changed (dirty), no other cache has a copy, OK to write, memory out-of-date, (Both the valid, and the dirty bit are set) 3. Exclusive: Up-to-date data, no other cache has a copy of it, OK to write, memory is up to date.          - If any other of the cache reads this line, the state will become <code>shared</code>         - If I were to write to this particular line, the state would become modified, but I don't need to tell any of the other caches that!         - Only valid is set 4. Owner: up-to-date data, other caches may have a copy (they must be in the Shared state), memory is NOT up to date.     - This cache can supplies data on a read instead of trying to go to the main memory. Saves the need for a write back when someone else reads the line     - Valid, Dirty, and shared bit is set     - When you write, you once again, have to have the other caches invalidate</p> <p>Conceptual Understanding:</p> <p>What is the name of the private cache that each processor core has?</p> <p>What is the cache coherence problem? It is the uniformity of shared resource data that ends up stored in multiple local caches.</p> <p>The Cache Coherence Problem:</p> <pre><code>It is the challenge of keeping multiple local caches *synchronized* when one of the processors updates its local copy of data which is shared among multiple caches.\n</code></pre> <p>Terminology that is really important to understand: </p> <p>The highlighted line right here is something referred to as a cache line, (i.e, cache block). We keep track of whatever the hell goes on with this particular component within our cache. </p> <p>Q: What is the difference between a cache line and a cache block? A: The difference between a cache line and a cache block is the following:</p> <p>Protocols to solve this problem: Snoopy protocol, this is where we essentially will look at whatever is going on within the interconnect and make sure that coherency is in check.</p> <p>Each core has a private cache.</p> <p> Any time that you want to interact with the main memory, you have gotta go through this bus, the other processors are gonna be listening because they have to ensure that they do not need to do anything regarding that particular request. (i.e, An example of this could be a read request, let's say I want to read a value from main memory) </p> <p>What happens when I try to update the value of a variable that is in a cache block that's currently in the <code>shared</code> state? What is the process of what happens? - The processor will send a upgrade request to the bus, the bus let's the other processor caches know that the upgrade is happening, and will make that block as invalid due to the fact that there is an inconsistency of what lies in that block amongst the caches now!</p> <p>The next lecture goes more in depth on this particular cache coherency protocol</p> <p>A write can be placed on the bus, and the <code>sharers</code> have to invalidate themselves. Links to external learning material to understand caching at a deeper level: 1. University of Washington \u2013 Cache Introduction https://courses.cs.washington.edu/courses/cse378/09wi/lectures/lec15.pdf 2. UC San Diego \u2013 The Basics of Caches https://cseweb.ucsd.edu/classes/su07/cse141/cache-handout.pdf 3. GeeksforGeeks \u2013 Cache Memory in Computer Organization https://www.geeksforgeeks.org/cache-memory-in-computer-organization/ 4. Wikipedia \u2013 CPU Cache https://en.wikipedia.org/wiki/CPU_cache 5. Swarthmore College \u2013 Cache Architecture and Design https://www.cs.swarthmore.edu/~kwebb/cs31/f18/memhierarchy/caching.html 6. Dive Into Systems \u2013 Caching https://diveintosystems.org/book/C11-MemHierarchy/caching.html</p>"},{"location":"Lecture%209/","title":"Lecture 9","text":""},{"location":"Lecture%209/#and-openmp","title":"$ and OpenMP","text":"<p>Every processor needs to have the same view of memory. Every thread shares the same view of memory!</p> <p>If a processor does a write, it has to let all of the other processors know. Invalidate the other copies. (Race Conditions only happen on writes). If we are only reading, many processors can have copies of said data. All of the other processors will 'snoop' on the interconnect and see any requests that go on the interconnect (i.e, usually a BUS) and see if it involves them in any way.  We never want all the writes to go back out to main memory, this is incredibly slow. This motivates us to have something called a write-back cache. Yeah, write-back is a jolly good fellow \ud83d\ude0e</p> <p>We also don't want to do a lot of writing in general. Even when using the write-back method. We can communicate by broadcasting requests. Using the bus allows us to communicate with the neighboring caches from the other processor / cores. </p> <p>Cache Optimization via these two new states: 3. Exclusive: up-to-date data, no other cache has a copy,  OK to write, memory is up to date     1. Only the Valid bit is set.  4. Owner: up to date data, other caches may have a copy, but within the other caches, it must be in the shared state! Main Memory is not up to date. </p>"},{"location":"Lecture%209/#easy-way-to-think-of-these-states","title":"Easy way to think of these states:","text":"<p>For the exclusive state, I found it the easiest to think of it like this: \u201cI have the only copy, it matches memory, and I can quietly turn it into a Modified copy whenever I want.\u201d</p> <p>For the owner state, a mental model: \u201cOthers can read from my data, but memory doesn\u2019t know what\u2019s up. If anyone needs the latest data, they come to me.\u201d</p>"},{"location":"Lecture%209/#idea-behind-the-states","title":"Idea behind the states:","text":"<ul> <li>M: Modified<ul> <li>I have the only copy, and can write, and it's dirty</li> <li>If this gets evicted, I need to flush the entry</li> </ul> </li> <li>O: Owned<ul> <li>I have the official copy, and can write, and it's dirty</li> <li>When writing, I have to tell everyone</li> <li>else I'm writing (And now the state will turn into Modified)</li> </ul> </li> <li>E: Exclusive<ul> <li>I have the only copy</li> </ul> </li> <li>S: Shared<ul> <li>I have a copy and can read away</li> </ul> </li> <li>I: Invalid<ul> <li>Duh, what do you think this means?</li> </ul> </li> </ul> <p>Weaver does a walk-through of what it looks like when each of the CPU cores wants to do an access for some data at a designated memory address w/ the cache coherency model.</p>"},{"location":"Lecture%209/#state-transition-diagram","title":"State Transition Diagram","text":"<p>High-level understanding: - All blocks start at the INVALID state, and transition throughout this diagram accordingly - Probe means that another Processor is asking for thingy Only one of the caches can have a block set as <code>Owner</code></p>"},{"location":"Lecture%209/#this-introduces-a-new-type-of-cache-miss-a-coherency-miss","title":"This introduces a new type of cache miss, a <code>coherency</code> miss:","text":"<p>If two CPU's are reading the same data, there is no problem at all. But, if one thread writes the data it will cause evictions from the other thread's cache.  It does not even need to occur with the specific same piece of data, it just has to be on the same cache line. If one processor is writing to the beginning of the cache line, and the other processor is reading to the beginning of the cache line, then the entire cache line is trashed. </p> <p>When it comes to multithreaded cores, capacity misses increase due to issues like incoherency. Incoherency occurs when we read or write different data. There is cache pressure that is being increased. This is still an issue!</p>"},{"location":"Lecture%209/#takeaways","title":"Takeaways...","text":"<p>Have each core work on separate pieces of data. Keep the data separate on a reasonable block size (256B will always be safe). Size for a cache about 1/2 of the actual size of the cache (if you face multithreaded CPU's)</p> <p>You want your working set to be half of what the actual cache size is. You also want <code>writes</code> that take place to be on disjoint boundaries. (i.e, What this means is that you do not one thread's writes to be in a place where another thread will be reading from. you want things to be separated effectively). </p> <p>What iscache thrashing?</p> <p>If there are multiple threads accessing the same cache, they are essentially putting capacity pressure on each other. This is not good whatsoever.</p>"},{"location":"Lecture%209/#analogy-notebooks","title":"Analogy ~ Notebooks!","text":"<p>Imagine that you and your friend (You are both CPUs/Cores), are both using a whiteboard (shared whiteboard) to keep track of each of your to-do lists. To save time, we each keep a little notebook (our respective caches), where we will copy the latest version of the information on the whiteboard. If the friend adds something to the to-do list, then we are gonna have to update our little notebook with what they added. This is b/c we still have the old version of what was on the whiteboard. Our system has to evict (erase) our old notebook page / update it so that we both agree.   For multithreaded cores, imagine that you and your friend share the same notebook, oof, we won't be able to keep track of all of these changes. There will be a lot of capacity misses, this is because the notebook cannot hold everything that we want it to hold.</p> <p>Capacity pressure \u2192 Capacity miss If that capacity pressure is because threads are using different working sets  \u2192 Professor calls that an \"incoherency miss\".</p>"},{"location":"Lecture%209/#openmp","title":"OpenMP","text":"<p>Language extension used for multi-threaded, shared memory parallelism. </p> <p>Key ideas: - shared vs. private variables</p> <p>Modeled around the fork-join model.</p> <p></p> <p>OpenMP begins as a single process (master thread), and it will execute sequentially until the very first parallel region construct is encountered. The <code>master</code> thread waits for all the worker threads to be done, and then, we join all of the threads together when we are done with the said parallel region. </p> <p>FORK: Master thread creates a team of parallel threads</p> <p>JOIN: When the team threads complete the statements in the parallel region construct, they synchronize and terminate, leaving only the master thread. </p> <p>Threads don't die, they go to sleep.</p> <p>Pragmas are a preprocessor mechanism C provides for language extensions. </p> <p>Good parallel code will still be able to run in a sequential system. </p> <pre><code>#pragma omp parallel\n{\n    /* code goes here*/\n}\n</code></pre>"},{"location":"cache%20thrashing/","title":"Cache thrashing","text":"<p>Cache thrash is an issue caused by an ongoing computer activity that fails to progress due to excessive use of resources or conflicts in the caching system. This flaw prevents client processes from taking advantage of the caching system. It can also lead to the undesirable eviction of useful data from the cache. Cache thrashing is a common problem in parallel processing architectures, where each central processing unit has its local cache.</p> <p>Link: https://www.techtarget.com/searchsoftwarequality/definition/cache-thrash</p>"},{"location":"interrupt%20handler/","title":"Interrupt handler","text":"<p>An interrupt handler is a routine that is executed by the processor in response to an interrupt signal. It is responsible for processing the interrupt and performing minimal processing tasks before deferring the rest of the processing to a deferred task. https://www.sciencedirect.com/topics/computer-science/interrupt-handler</p>"},{"location":"poisoned/","title":"Poisoned","text":"<p>If a thread panics while holding a lock, Rust marks the lock as \"poisoned.\"</p>"},{"location":"queue/","title":"Queue","text":"<p>A queue is an abstract data type that holds an ordered, linear sequence of items. You can describe it as a first in, first out (FIFO) structure; the first element to be added to the queue will be the first element to be removed from the queue. New elements are added to the back or rear of the queue. When an element is removed, the remaining elements do not move up to take the empty space. To keep the order of the queue you need to maintain a pointer to the front, which indicates the element at the front of the queue, and one to the rear, which indicates the element at the back of the queue. </p>"},{"location":"blog/","title":"Blog","text":""}]}